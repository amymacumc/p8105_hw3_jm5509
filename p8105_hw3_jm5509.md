p8105_hw3_jm5509
================
Echo
2022-10-07

## Problem 1

This problem uses the Instacart data.The goal is to do some exploration
of this dataset. How many aisles are there, and which aisles are the
most items ordered from?

``` r
data("instacart")
(instacart %>% group_by(aisle) %>% count() %>% nrow())
```

    ## [1] 134

``` r
(instacart %>% group_by(aisle_id) %>% count() %>% nrow())
```

    ## [1] 134

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>% group_by(aisle) %>% 
  summarize(n_ordered = n()) %>% 
  filter(n_ordered > 10000) %>% 
  arrange(desc(n_ordered)) %>% 
  mutate(aisle = fct_reorder(aisle, n_ordered)) %>% 
  ggplot(aes(y=aisle, x= n_ordered, fill=aisle))+
  geom_col()+
  labs(
     title = "Number of items ordered in every aisle",
      x = "Aisle",
      y = "Number of items ordered") +
  theme(legend.position = "none")
```

![](p8105_hw3_jm5509_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart %>% 
  filter(aisle == c('baking ingredients', 
                    'dog food care', 'packaged vegetables fruits')) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_product = n()) %>% 
  filter(min_rank(desc(n_product)) <= 3) %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the
    ## `.groups` argument.

| aisle                      | product_name                                    | n_product |
|:---------------------------|:------------------------------------------------|----------:|
| baking ingredients         | Light Brown Sugar                               |       157 |
| baking ingredients         | Organic Vanilla Extract                         |       122 |
| baking ingredients         | Pure Baking Soda                                |       140 |
| dog food care              | Organix Chicken & Brown Rice Recipe             |        13 |
| dog food care              | Organix Grain Free Chicken & Vegetable Dog Food |        14 |
| dog food care              | Original Dry Dog                                |         9 |
| packaged vegetables fruits | Organic Baby Spinach                            |      3324 |
| packaged vegetables fruits | Organic Blueberries                             |      1692 |
| packaged vegetables fruits | Organic Raspberries                             |      1920 |

Finally is a table showing the mean hour of the day at which Pink Lady
Apples and Coffee Ice Cream are ordered on each day of the week. This
table has been formatted in an untidy manner for human readers. Pink
Lady Apples are generally purchased slightly earlier in the day than
Coffee Ice Cream, with the exception of day 5.

``` r
instacart %>% 
  filter(product_name == c('Pink Lady Apples','Coffee Ice Cream')) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    mean_order_hour = mean(order_hour_of_day)
  ) %>% 
  mutate(order_dow = recode(order_dow, `0` = 'Sunday', `1` =' Monday',
                            `2` = 'Tuesday', `3` = 'Wednesday',
                            `4` = 'Thursday', `5` = 'Friday',
                            `6` = 'Saturday')) %>% 
  pivot_wider(names_from = order_dow, values_from = mean_order_hour) %>% 
  knitr::kable()
```

    ## Warning in product_name == c("Pink Lady Apples", "Coffee Ice Cream"): longer
    ## object length is not a multiple of shorter object length

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

| product_name     |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
|:-----------------|---------:|---------:|---------:|----------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.22222 | 15.00000 | 15.33333 |   15.4000 | 15.16667 | 10.33333 | 12.35294 |
| Pink Lady Apples | 12.25000 | 11.67857 | 12.00000 |   13.9375 | 11.90909 | 13.86957 | 11.55556 |

## Problem 2

This question is about dealing with the accelerometers data. I would
read the data first, and tidying it by adding one a weekday vs weekend
variable, and changing the variables of activity every minute into
observations by *pivot_longer()* function. In this case this dataframe
would have useful names.

``` r
(accel_df <- read_csv('accel_data.csv') %>% 
  janitor::clean_names() %>% 
  rename_with(~str_replace(., regex('^activity_'),'')) %>% 
  mutate(weekday_vs_weekend =
           ifelse(day %in% c('Monday', 'Tuesday', 'Wednesday',
                             'Thursday', 'Friday'), 'weekday', 'weekend')) %>% 
  pivot_longer('1':'1440', names_to = 'minute', values_to = 'activity') %>% 
  mutate(minute = as.integer(minute)) 
 )
```

    ## Rows: 35 Columns: 1443
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    ## # A tibble: 50,400 × 6
    ##     week day_id day    weekday_vs_weekend minute activity
    ##    <dbl>  <dbl> <chr>  <chr>               <int>    <dbl>
    ##  1     1      1 Friday weekday                 1     88.4
    ##  2     1      1 Friday weekday                 2     82.2
    ##  3     1      1 Friday weekday                 3     64.4
    ##  4     1      1 Friday weekday                 4     70.0
    ##  5     1      1 Friday weekday                 5     75.0
    ##  6     1      1 Friday weekday                 6     66.3
    ##  7     1      1 Friday weekday                 7     53.8
    ##  8     1      1 Friday weekday                 8     47.8
    ##  9     1      1 Friday weekday                 9     55.5
    ## 10     1      1 Friday weekday                10     43.0
    ## # … with 50,390 more rows

This produces a dataframe with **50400** observations and **6**
variables. The variables are **week, day_id, day, weekday_vs_weekend,
minute, activity**.

To better use the activity data, we can aggregate across minutes to
create a total activity variable for each day. Then I plot these
activity per day to see if there is an activity trend.

``` r
accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity))
```

    ## `summarise()` has grouped output by 'week', 'day_id'. You can override using
    ## the `.groups` argument.

    ## # A tibble: 35 × 4
    ## # Groups:   week, day_id [35]
    ##     week day_id day       total_activity
    ##    <dbl>  <dbl> <chr>              <dbl>
    ##  1     1      1 Friday           480543.
    ##  2     1      2 Monday            78828.
    ##  3     1      3 Saturday         376254 
    ##  4     1      4 Sunday           631105 
    ##  5     1      5 Thursday         355924.
    ##  6     1      6 Tuesday          307094.
    ##  7     1      7 Wednesday        340115.
    ##  8     2      8 Friday           568839 
    ##  9     2      9 Monday           295431 
    ## 10     2     10 Saturday         607175 
    ## # … with 25 more rows

``` r
accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  ggplot(aes(x=day, y = total_activity, fill = week)) +
  geom_col(position = 'dodge2') +
  labs(
     title = "Trends of a total activity for each day",
      x = "Day",
      y = "Activity") 
```

    ## `summarise()` has grouped output by 'week', 'day_id'. You can override using
    ## the `.groups` argument.

![](p8105_hw3_jm5509_files/figure-gfm/aggregate%20and%20plot-1.png)<!-- -->

``` r
accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  ggplot(aes(x=day_id, y = total_activity, color = day))+
  geom_point()+geom_line()
```

    ## `summarise()` has grouped output by 'week', 'day_id'. You can override using
    ## the `.groups` argument.

![](p8105_hw3_jm5509_files/figure-gfm/aggregate%20and%20plot-2.png)<!-- -->

From the column chart and the line chart above, there is no apparent
within-week and between-week trend. However, there are some outliers
that the last two Saturdays’ activities are obviously lower than any
other days, which is 1440. Both the within-week and the between-week
activity vary a lot.

I would make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week.

``` r
accel_df %>% 
  ggplot(aes(x=minute, y = activity, fill = day))+
  geom_bar(stat = 'identity')+
  scale_x_continuous(breaks = seq(0,1440, 60)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
     title = "24-hour activity time courses for each day",
      x = "Minute",
      y = "Activity") 
```

![](p8105_hw3_jm5509_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

From this panel, we can see that the activity tend do be higher in
Fridays and Mondays, and lower on Wednesdays and Tuesdays. During the
one-day period, the activity tend to be higher after 7:00 am.(Suppose
that it’s because the subject is at sleep.) The activity peak is at
around 11:00 and 20:00.

## Problem 3

The following code was used to acquire the five core variables for all
New York state weather stations from January 1, 1981 through December
31, 2010. I would first read the data and then do some data cleaning: I
create separate variables for year, month, and day and ensure
observations for temperature, precipitation, and snowfall are given in
reasonable units.

``` r
data('ny_noaa')
(ny_noaa <- ny_noaa %>% 
  separate(date, into = c('year', 'month', 'day'), sep='-') %>% 
  mutate(prcp = prcp/10, tmax = as.numeric(tmax)/ 10, 
         tmin = as.numeric(tmin)/10))
```

    ## # A tibble: 2,595,176 × 9
    ##    id          year  month day    prcp  snow  snwd  tmax  tmin
    ##    <chr>       <chr> <chr> <chr> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001 2007  11    01       NA    NA    NA    NA    NA
    ##  2 US1NYAB0001 2007  11    02       NA    NA    NA    NA    NA
    ##  3 US1NYAB0001 2007  11    03       NA    NA    NA    NA    NA
    ##  4 US1NYAB0001 2007  11    04       NA    NA    NA    NA    NA
    ##  5 US1NYAB0001 2007  11    05       NA    NA    NA    NA    NA
    ##  6 US1NYAB0001 2007  11    06       NA    NA    NA    NA    NA
    ##  7 US1NYAB0001 2007  11    07       NA    NA    NA    NA    NA
    ##  8 US1NYAB0001 2007  11    08       NA    NA    NA    NA    NA
    ##  9 US1NYAB0001 2007  11    09       NA    NA    NA    NA    NA
    ## 10 US1NYAB0001 2007  11    10       NA    NA    NA    NA    NA
    ## # … with 2,595,166 more rows

The original dataframe has **2595176 observations** and **9 variables**.
The key variables include date(date of observation), prcp(Precipitation
(tenths of mm)), snow(snowfall(mm)), tmax(Maximum temperature (tenths of
degrees C)), tmin(Minimum temperature (tenths of degrees C)). However,
it contains many missing values. There is 145838 missing precipitations,
381221 missing snowfall, 591786 missing snow depth, 1134358 missing
maximum temperature, 1134420 minimum temperature. After dropping all the
missing values, the dataset contains 1222433 observations.

For snowfall, what are the most commonly observed values?

``` r
ny_noaa %>% drop_na(snow) %>% 
  mutate(year = as.numeric(year), month = as.numeric(month), 
         day = as.numeric(day)
         ) %>% 
  group_by(snow) %>% 
  summarize(n_snow = n()) %>% 
  arrange(desc(n_snow)) %>% 
  top_n(5)
```

    ## Selecting by n_snow

    ## # A tibble: 5 × 2
    ##    snow  n_snow
    ##   <int>   <int>
    ## 1     0 2008508
    ## 2    25   31022
    ## 3    13   23095
    ## 4    51   18274
    ## 5    76   10173

The most commonly observed snowfall values are
`0, 25, 13, 51, and 76mm`.

Make a two-panel plot showing the average max temperature in January and
in July in each station across years. Is there any observable /
interpretable structure? Any outliers?

``` r
ny_noaa %>% drop_na(tmax) %>% 
  mutate(year = as.numeric(year), month = as.numeric(month),
         day = as.numeric(day)
         ) %>% 
  filter(month == c(1,7)) %>% 
  group_by(id, year, month) %>% 
  summarize(tmax_mean = mean(tmax)) %>% 
  mutate(month = month.name[month]) %>% 
  ggplot(aes(x = year, y = tmax_mean, color = id)) +
  geom_point()+geom_line()+facet_grid(~month)+
  scale_x_continuous(breaks = seq(1980, 2010, 5)) +
    theme(legend.position = "none") +
    labs(
     title = "the average tmax in January and in July in each station across years",
      x = "year",
      y = "average tmax") 
```

    ## `summarise()` has grouped output by 'id', 'year'. You can override using the
    ## `.groups` argument.

![](p8105_hw3_jm5509_files/figure-gfm/t%20in%20January%20and%20July-1.png)<!-- -->

From this plot, we can identify that the January data fluctuate around
-10 to 5, and July data fluctuate around 22 to 30 centigrade. January
data has higher year-to-year and within-year fluctuation than July data.
Outliers are more visible in the July panel. The most significant
outlier is noticed in July 1988. Other outlier dates include July 1984
and July 2004.

Make a two-panel plot showing (i) tmax vs tmin for the full dataset
(note that a scatterplot may not be the best option); and (ii) make a
plot showing the distribution of snowfall values greater than 0 and less
than 100 separately by year.

``` r
tplot <- ny_noaa %>%  drop_na(tmax, tmin) %>% 
  ggplot(aes(x = tmin, y = tmax))+
   geom_hex(bins = 50) +geom_smooth()+ 
  scale_x_continuous(breaks = seq(1980, 2010, 5)) +
  theme(legend.position = "none") 
  
snowplot <- ny_noaa %>% drop_na(snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) +
  geom_violin(alpha = 0.5)+
 theme(legend.position = "none", axis.text.x = element_text(angle = 90))

tplot / snowplot
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw3_jm5509_files/figure-gfm/tmax%20and%20tmin-1.png)<!-- -->
