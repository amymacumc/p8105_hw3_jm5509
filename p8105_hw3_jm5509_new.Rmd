---
title: "p8105_hw3_jm5509"
author: "Echo"
date: "2022-10-15"
output: github_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(lubridate)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
```

## Problem 1
This problem uses the Instacart data.The goal is to do some exploration of this dataset.
How many aisles are there, and which aisles are the most items ordered from?
```{r}
data("instacart")
(instacart %>% group_by(aisle) %>% count() %>% nrow())
```

```{r}
(instacart %>% group_by(aisle_id) %>% count() %>% nrow())
```
Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
instacart %>% group_by(aisle) %>% 
  summarize(n_ordered = n()) %>% 
  filter(n_ordered > 10000) %>% 
  arrange(desc(n_ordered)) %>% 
  mutate(aisle = fct_reorder(aisle, n_ordered)) %>% 
  ggplot(aes(y=aisle, x= n_ordered, fill=aisle))+
  geom_col()+
  labs(
     title = "Number of items ordered in every aisle",
      x = "Aisle",
      y = "Number of items ordered") +
  theme(legend.position = "none")
```
Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>% 
  filter(aisle == c('baking ingredients', 
                    'dog food care', 'packaged vegetables fruits')) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_product = n()) %>% 
  filter(min_rank(desc(n_product)) <= 3) %>% 
  knitr::kable()
```
As is shown above, the top 3 baking ingredients are `light brown sugar`, `organic vanilla extract`, and `pure baking soda`; the top 3 dog food cares are `Organix Chicken & Brown Rice Recipe`, `Organix Grain Free Chicken & Vegetable Dog Food`, and `Original Dry Dog`; the top 3 packaged vegetables fruits are `Organic Baby Spinach`, `Organic Blueberries`, `Organic Raspberries`. Comparing the sales volumes, the `packaged vegetables fruits` category is the most popular one among the three.

Finally, for each day of the week, a table shows the average hour of the day when Pink Lady Apples and Coffee Ice Cream are ordered. For human readers, this table has been structured in an untidy manner. With the exception of day 5, Pink Lady Apples are normally purchased significantly earlier in the day than Coffee Ice Cream.
```{r}
instacart %>% 
  filter(product_name == c('Pink Lady Apples','Coffee Ice Cream')) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    mean_order_hour = mean(order_hour_of_day)
  ) %>% 
  mutate(order_dow = recode(order_dow, `0` = 'Sunday', `1` =' Monday',
                            `2` = 'Tuesday', `3` = 'Wednesday',
                            `4` = 'Thursday', `5` = 'Friday',
                            `6` = 'Saturday')) %>% 
  pivot_wider(names_from = order_dow, values_from = mean_order_hour) %>% 
  knitr::kable()
```
## Problem 2
This question is about dealing with the accelerometers data. I would read the data first, 
and tidying it by adding one  a weekday vs weekend variable, and changing the variables of activity every minute into observations by *pivot_longer()* function. In this case this dataframe
would have useful names.
```{r accel_df}
(accel_df <- read_csv('accel_data.csv') %>% 
  janitor::clean_names() %>% 
  rename_with(~str_replace(., regex('^activity_'),'')) %>% 
  mutate(weekday_vs_weekend =
           ifelse(day %in% c('Monday', 'Tuesday', 'Wednesday',
                             'Thursday', 'Friday'), 'weekday', 'weekend')) %>% 
  pivot_longer('1':'1440', names_to = 'minute', values_to = 'activity') %>% 
  mutate(minute = as.integer(minute)) 
 )
```
This produces a dataframe with **`r nrow(accel_df)`** observations and **`r ncol(accel_df)`** variables. The variables are **`r colnames(accel_df)`**.

To better use the activity data, we can aggregate across minutes to create a total activity variable for each day. Then I plot these activity per day to see if there is an activity trend.
```{r aggregate and plot}

accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity))

accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  ggplot(aes(x=day, y = total_activity, fill = week)) +
  geom_col(position = 'dodge2') +
  labs(
     title = "Trends of a total activity for each day",
      x = "Day",
      y = "Activity") 

accel_df %>% group_by(week, day_id, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  ggplot(aes(x=day_id, y = total_activity, color = day))+
  geom_point()+geom_line()
```

From the column chart and the line chart above, there is no apparent within-week and between-week trend. However, there are some outliers that the last two Saturdays' activities are obviously lower than any other days, which is 1440. Both the within-week and the between-week activity vary a lot.

I would make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.
```{r}
accel_df %>% 
  ggplot(aes(x=minute, y = activity, fill = day))+
  geom_bar(stat = 'identity')+
  scale_x_continuous(breaks = seq(0,1440, 60)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
     title = "24-hour activity time courses for each day",
      x = "Minute",
      y = "Activity") 
  
```

From this panel, we can see that the activity tend do be higher in Fridays and Mondays, and lower on Wednesdays and Tuesdays. During the one-day period, the activity tend to be higher after 7:00 am.(Suppose that it's because the subject is at sleep.) The activity peak is at around 11:00 and 20:00.

## Problem 3
The following code was used to acquire the five core variables for all New York state weather stations from January 1, 1981 through December 31, 2010. I would first read the data and then do some data cleaning: I create separate variables for year, month, and day and ensure observations for temperature, precipitation, and snowfall are given in reasonable units.
```{r}
data('ny_noaa')
(ny_noaa <- ny_noaa %>% 
  separate(date, into = c('year', 'month', 'day'), sep='-') %>% 
  mutate(prcp = prcp/10, tmax = as.numeric(tmax)/ 10, 
         tmin = as.numeric(tmin)/10))
```
The original dataframe has **`r ny_noaa %>% nrow()` observations** and **`r ny_noaa  %>% ncol()` variables**. The key variables include date(date of observation), prcp(Precipitation (tenths of mm)), snow(snowfall(mm)), tmax(Maximum temperature (tenths of degrees C)), tmin(Minimum temperature (tenths of degrees C)).
However, it contains many missing values. There is `r sum(is.na(ny_noaa$prcp))` missing precipitations, `r sum(is.na(ny_noaa$snow))` missing snowfall, `r sum(is.na(ny_noaa$snwd))` missing snow depth, `r sum(is.na(ny_noaa$tmax))` missing maximum temperature, `r sum(is.na(ny_noaa$tmin))` minimum temperature. After dropping all the missing values, the dataset contains `r ny_noaa %>% na.omit(prcp:tmin)  %>% nrow()` observations.

